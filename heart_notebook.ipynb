{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab79867-82e0-4556-9408-58a85178843f",
   "metadata": {},
   "source": [
    "# Heart Attacks are Funny\n",
    "\n",
    "This is some analysis to save Rich Evans' life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e365e14b-f42a-4f2b-871e-e6470d900a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso#Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587efd02-a329-47f8-8efc-947c4399ae9c",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27afdf8a-037c-442f-8dcb-7a7829364b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7a393-b318-44bc-84d5-ddfb3d41cbe9",
   "metadata": {},
   "source": [
    "## Show me that sweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b796b-8822-497e-92d0-ccb917151e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d269c-c408-4530-bac7-f3a8d3030ccb",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "1. age - age in years\n",
    "2. sex - sex (1 = male; 0 = female)\n",
    "3. cp - chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 0 = asymptomatic)\n",
    "4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n",
    "5. chol - serum cholestoral in mg/dl\n",
    "6. fbs - fasting blood sugar > 120 mg/dl (1 = true; 0 = false)\n",
    "7. restecg - resting electrocardiographic results (0 = normal; 1 = having ST-T; 2 = hypertrophy)\n",
    "8. thalach - maximum heart rate achieved\n",
    "9. exng - exercise induced angina (1 = yes; 0 = no)\n",
    "10. oldpeak - ST depression induced by exercise relative to rest\n",
    "11. slope - the slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping)\n",
    "12. ca - number of major vessels (0-3) colored by flourosopy\n",
    "13. thal - 2 = normal; 1 = fixed defect; 3 = reversable defect\n",
    "14. num - the predicted attribute - diagnosis of heart disease (angiographic disease status) (Value 0 = < diameter narrowing; Value 1 = > 50% diameter narrowing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce4a5e0-f7a2-4e81-a5ea-7bae61203790",
   "metadata": {},
   "source": [
    "Looks like the previous sample had a lot of people who are gonna die. Let's see some live Rich Evans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a933dc-fd66-4d1d-a20e-ceaf8f0a915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_hearts = heart_df.where(heart_df['output'] == 0).dropna()\n",
    "good_hearts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a3883-cbb2-4c9d-9ded-1be8bf3d58d3",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Let's do some feature selection. Inspired by [this blog](https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7479a8a-f036-46ad-8863-d55b10bd5489",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart_df.drop('output', axis=1)   #Feature Matrix\n",
    "y = heart_df['output']          #Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac9185c-ddc0-46a6-8e91-d12cb4f08e35",
   "metadata": {},
   "source": [
    "### Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e828f36-1b09-41d0-a4af-727632ec2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pearson Correlation\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = heart_df.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e15d2-3b95-4557-8648-9d5279208383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation with output variable\n",
    "cor_target = abs(cor['output'])\n",
    "\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.4]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2c991-4531-4d13-96fd-871720967379",
   "metadata": {},
   "source": [
    "## Initial thoughts\n",
    "\n",
    "Chest pain, max heart rate, angia from exercise, and ST depression all mean Rich is gonna die. Makes sense. We better hope he don't got none of that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff6ed31-4cfd-4ef2-81cd-1fc6c3a1a88f",
   "metadata": {},
   "source": [
    "One of the assumptions of linear regression is that the independent variables need to be uncorrelated with each other. If these variables are correlated with each other, then we need to keep only one of them and drop the rest. So let us check the correlation of selected features with each other. This can be done either by visually checking it from the above correlation matrix or from the code snippet below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d4f7eb-30cc-49af-a383-f1d141287fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heart_df[[\"cp\",\"thalachh\"]].corr())\n",
    "print(heart_df[[\"cp\",\"exng\"]].corr())\n",
    "print(heart_df[[\"cp\",\"oldpeak\"]].corr())\n",
    "print(heart_df[[\"exng\",\"thalachh\"]].corr())\n",
    "print(heart_df[[\"exng\",\"oldpeak\"]].corr())\n",
    "print(heart_df[[\"thalachh\",\"oldpeak\"]].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5072a1-b7ed-4ccb-b6b8-ea497ec0037b",
   "metadata": {},
   "source": [
    "There are no correlations above 0.5 so we will keep all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca210ba-084b-4411-87d5-00ad510718b4",
   "metadata": {},
   "source": [
    "### Backward Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e368fd90-7383-4324-807f-ab5e6adeb540",
   "metadata": {},
   "source": [
    "Using a simple model (Ordinary Least Squares), feed all features to model. Then iteratively remove worst feature until we reach the desired performance to feature ratio. We will use a features p-value to determine whether it will save Mr. Evans' life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4a91c-aaee-4dfc-a837-840c2c6d5c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding constant column of ones, mandatory for sm.OLS model\n",
    "X_1 = sm.add_constant(X)\n",
    "\n",
    "#Fitting sm.OLS model\n",
    "model = sm.OLS(y, X_1).fit()\n",
    "model.pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5d6ed3-e25e-4236-b114-8e55bc987d86",
   "metadata": {},
   "source": [
    "A lower p-value means more statistical significance, so we remove the features with p-value > 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1bbb14-4582-4ade-b69c-c588527e2672",
   "metadata": {},
   "source": [
    "Now we loop training the model, checking the max p-value, and removing it until there are no features with p-value < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a0be7-a05c-4c0e-8045-4f314099d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward Elimination\n",
    "cols = list(X.columns)\n",
    "pmax = 1\n",
    "while(len(cols) > 0):\n",
    "    p= []\n",
    "    X_1 = X[cols]\n",
    "    X_1 = sm.add_constant(X_1)\n",
    "    model = sm.OLS(y,X_1).fit()\n",
    "    p = pd.Series(model.pvalues.values[1:],index = cols)      \n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax > 0.05):\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "selected_features_BE = cols\n",
    "print(selected_features_BE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907c3b94-aa19-4898-ab44-2c28b36eef6c",
   "metadata": {},
   "source": [
    "Notice sex is in this feature set while not in the feature set which was selected via Pearson Correlation. It is possible that looking at the correlations of all features at once and selecting may give a worse candidate set of features versus knowing the right subset. This "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929015d6-72a5-4874-9228-b4e8c1882626",
   "metadata": {},
   "source": [
    "### RFE\n",
    "\n",
    "A more sophisticated feature elimination algorithm is the Recursive Feature Elimination (RFE) method which works by recursively removing attributes and building a model on those attributes that remain. It uses accuracy metric to rank the feature according to their importance. The RFE method takes the model to be used and the number of required features as input. It then gives the ranking of all the variables, 1 being most important. It also gives its support, True being relevant feature and False being irrelevant feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38e74f-c0e0-4b6c-9d35-491226971df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "#Initializing RFE model\n",
    "rfe = RFE(model, 7)\n",
    "\n",
    "#Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X,y)\n",
    "\n",
    "#Fitting the data to model\n",
    "model.fit(X_rfe,y)\n",
    "\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c9d92-77f7-4f8d-9ab6-abac57f5a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no of features\n",
    "nof_list=np.arange(1,13)            \n",
    "high_score=0\n",
    "#Variable to store the optimum features\n",
    "nof=0           \n",
    "score_list =[]\n",
    "for n in range(len(nof_list)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model,nof_list[n])\n",
    "    X_train_rfe = rfe.fit_transform(X_train,y_train)\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    model.fit(X_train_rfe,y_train)\n",
    "    score = model.score(X_test_rfe,y_test)\n",
    "    score_list.append(score)\n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        nof = nof_list[n]\n",
    "        \n",
    "print(\"Optimum number of features: %d\" %nof)\n",
    "print(\"Score with %d features: %f\" % (nof, high_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56717d5-7df7-4f4c-880b-16f8ffa2a169",
   "metadata": {},
   "source": [
    "The RFE is the most liberal of all the feature selection algorithms so far. It found the best performance using linear regression on a subset of features selected based on the number of features desired was with 11/13 features. Now let's fit the regression model with the number of features suggested and find out which features were selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3929b34a-4a0c-4413-ac78-208908a34d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(X.columns)\n",
    "model = LinearRegression()\n",
    "\n",
    "#Initializing RFE model\n",
    "rfe = RFE(model, 10)\n",
    "\n",
    "#Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X,y)\n",
    "\n",
    "#Fitting the data to model\n",
    "model.fit(X_rfe,y)              \n",
    "was_selected_array = pd.Series(rfe.support_, index = cols)\n",
    "selected_features_rfe = temp[temp==True].index\n",
    "print(selected_features_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acc68d5-8473-40e1-b53a-fd6d85cfb9d1",
   "metadata": {},
   "source": [
    "### Embedded Methods\n",
    "\n",
    "This group of algorithms uses regularization methods to select features during training. We'll try with the Ridge regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af9d7a-256a-4265-b6d2-6dd6bee42060",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RidgeCV()\n",
    "reg.fit(X, y)\n",
    "print(\"Best alpha using built-in RidgeCV: %f\" % reg.alpha_)\n",
    "print(\"Best score using built-in RidgeCV: %f\" %reg.score(X,y))\n",
    "coef = pd.Series(reg.coef_, index = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11681149-4655-441d-bc34-748b56170684",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe0278-4907-4cfc-ace4-bb8be610a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_coef = coef.sort_values()\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Feature importance using Ridge Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bf2d55-0c27-4edf-a155-2bc5da8a1ec7",
   "metadata": {},
   "source": [
    "Well children, we regularized Rich Evans' heart. We done good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cedab4-895a-4012-9837-cddf200da970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
